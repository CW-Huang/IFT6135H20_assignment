{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae_template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC71Qv1-TtI-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Solution template for the question 1.6-1.7. This template consists of following steps. Except the step 2, you don't need to modify it to answer the questions.\n",
        "1.   Initialize libraries\n",
        "2.   **Insert the answers for the questions 1.1~1.5 below (this is the part you need to fill)**\n",
        "3.   Define data loaders\n",
        "4.   Define VAE network architecture\n",
        "5.   Initialize the model and optimizer\n",
        "6.   Train the model\n",
        "7.   Save the model\n",
        "8.   Load the model\n",
        "9.   Evaluate the model with importance sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcs7QFvETxQJ",
        "colab_type": "text"
      },
      "source": [
        "Initialize libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLoP5GRpEPbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from torchvision.datasets import utils\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn.modules import upsampling\n",
        "from torch.functional import F\n",
        "from torch.optim import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTB40neeR6-k",
        "colab_type": "text"
      },
      "source": [
        "Insert **the answers for the questions 1.1~1.5 below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kr08AArNlHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_likelihood_bernoulli(mu, target):\n",
        "    \"\"\" \n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "    :param mu: (FloatTensor) - shape: (batch_size x input_size) - The mean of Bernoulli random variables p(x=1).\n",
        "    :param target: (FloatTensor) - shape: (batch_size x input_size) - Target samples (binary values).\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - log-likelihood of target samples on the Bernoulli random variables.\n",
        "    \"\"\"\n",
        "    # init\n",
        "    batch_size = mu.size(0)\n",
        "    mu = mu.view(batch_size, -1)\n",
        "    target = target.view(batch_size, -1)\n",
        "\n",
        "    # log_likelihood_bernoulli\n",
        "    return \n",
        "\n",
        "\n",
        "def log_likelihood_normal(mu, logvar, z):\n",
        "    \"\"\" \n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "    :param mu: (FloatTensor) - shape: (batch_size x input_size) - The mean of Normal distributions.\n",
        "    :param logvar: (FloatTensor) - shape: (batch_size x input_size) - The log variance of Normal distributions.\n",
        "    :param z: (FloatTensor) - shape: (batch_size x input_size) - Target samples.\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - log probability of the sames on the given Normal distributions.\n",
        "    \"\"\"\n",
        "    # init\n",
        "    batch_size = mu.size(0)\n",
        "    mu = mu.view(batch_size, -1)\n",
        "    logvar = logvar.view(batch_size, -1)\n",
        "    z = z.view(batch_size, -1)\n",
        "\n",
        "    # log normal\n",
        "    return \n",
        "\n",
        "\n",
        "def log_mean_exp(y):\n",
        "    \"\"\" \n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "    :param y: (FloatTensor) - shape: (batch_size x sample_size) - Values to be evaluated for log_mean_exp. For example log proababilies\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - Output for log_mean_exp.\n",
        "    \"\"\"\n",
        "    # init\n",
        "    batch_size = y.size(0)\n",
        "    sample_size = y.size(1)\n",
        "\n",
        "    # log_mean_exp\n",
        "    return \n",
        "\n",
        "\n",
        "def kl_gaussian_gaussian_analytic(mu_q, logvar_q, mu_p, logvar_p):\n",
        "    \"\"\" \n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "    :param mu_q: (FloatTensor) - shape: (batch_size x input_size) - The mean of first distributions (Normal distributions).\n",
        "    :param logvar_q: (FloatTensor) - shape: (batch_size x input_size) - The log variance of first distributions (Normal distributions).\n",
        "    :param mu_p: (FloatTensor) - shape: (batch_size x input_size) - The mean of second distributions (Normal distributions).\n",
        "    :param logvar_p: (FloatTensor) - shape: (batch_size x input_size) - The log variance of second distributions (Normal distributions).\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - kl-divergence of KL(q||p).\n",
        "    \"\"\"\n",
        "    # init\n",
        "    batch_size = mu_q.size(0)\n",
        "    mu_q = mu_q.view(batch_size, -1)\n",
        "    logvar_q = logvar_q.view(batch_size, -1)\n",
        "    mu_p = mu_p.view(batch_size, -1)\n",
        "    logvar_p = logvar_p.view(batch_size, -1)\n",
        "\n",
        "    # kld\n",
        "    return \n",
        "\n",
        "\n",
        "def kl_gaussian_gaussian_mc(mu_q, logvar_q, mu_p, logvar_p, num_samples=1):\n",
        "    \"\"\" \n",
        "    COMPLETE ME. DONT MODIFY THE PARAMETERS OF THE FUNCTION. Otherwise, tests might fail.\n",
        "\n",
        "    *** note. ***\n",
        "\n",
        "    :param mu_q: (FloatTensor) - shape: (batch_size x input_size) - The mean of first distributions (Normal distributions).\n",
        "    :param logvar_q: (FloatTensor) - shape: (batch_size x input_size) - The log variance of first distributions (Normal distributions).\n",
        "    :param mu_p: (FloatTensor) - shape: (batch_size x input_size) - The mean of second distributions (Normal distributions).\n",
        "    :param logvar_p: (FloatTensor) - shape: (batch_size x input_size) - The log variance of second distributions (Normal distributions).\n",
        "    :param num_samples: (int) - shape: () - The number of sample for Monte Carlo estimate for KL-divergence\n",
        "    :return: (FloatTensor) - shape: (batch_size,) - kl-divergence of KL(q||p).\n",
        "    \"\"\"\n",
        "    # init\n",
        "    batch_size = mu_q.size(0)\n",
        "    input_size = np.prod(mu_q.size()[1:])\n",
        "    mu_q = mu_q.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "    logvar_q = logvar_q.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "    mu_p = mu_p.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "    logvar_p = logvar_p.view(batch_size, -1).unsqueeze(1).expand(batch_size, num_samples, input_size)\n",
        "\n",
        "    # kld\n",
        "    return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3v_ld3ITRFl",
        "colab_type": "text"
      },
      "source": [
        "Define data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiK4L0TdETNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_loader(dataset_location, batch_size):\n",
        "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
        "    # start processing\n",
        "    def lines_to_np_array(lines):\n",
        "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
        "    splitdata = []\n",
        "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
        "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
        "        filepath = os.path.join(dataset_location, filename)\n",
        "        utils.download_url(URL + filename, dataset_location)\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "        x = lines_to_np_array(lines).astype('float32')\n",
        "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
        "        # pytorch data loader\n",
        "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
        "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
        "        splitdata.append(dataset_loader)\n",
        "    return splitdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZsL1gLLEVJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, valid, test = get_data_loader(\"binarized_mnist\", 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PoFxey7TUFS",
        "colab_type": "text"
      },
      "source": [
        "Define VAE network architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POBmU6UCEb4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(784, 300),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(300, 300),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(300, 2 * latent_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        z_mean, z_logvar = self.mlp(x.view(batch_size, 784)).chunk(2, dim=-1)\n",
        "        return z_mean, z_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(latent_size, 300),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(300, 300),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(300, 784),\n",
        "        )\n",
        "        \n",
        "    def forward(self, z):\n",
        "        return self.mlp(z) - 5.\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_size):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encode = Encoder(latent_size)\n",
        "        self.decode = Decoder(latent_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z_mean, z_logvar = self.encode(x)\n",
        "        z_sample = z_mean + torch.exp(z_logvar / 2.) * torch.randn_like(z_logvar)\n",
        "        x_mean = self.decode(z_sample)\n",
        "        return z_mean, z_logvar, x_mean\n",
        "\n",
        "    def loss(self, x, z_mean, z_logvar, x_mean):\n",
        "        ZERO = torch.zeros(z_mean.size())\n",
        "        #kl = kl_gaussian_gaussian_mc(z_mean, z_logvar, ZERO, ZERO, num_samples=1000).mean()\n",
        "        kl = kl_gaussian_gaussian_analytic(z_mean, z_logvar, ZERO, ZERO).mean()\n",
        "        recon_loss = -log_likelihood_bernoulli(\n",
        "            torch.sigmoid(x_mean.view(x.size(0), -1)),\n",
        "            x.view(x.size(0), -1),            \n",
        "        ).mean()\n",
        "        return recon_loss + kl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phg07ERvTYuh",
        "colab_type": "text"
      },
      "source": [
        "Initialize a model and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTxgDwZfEesO",
        "colab_type": "code",
        "outputId": "4d0db765-9306-4b84-e66b-fe8403567623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "vae = VAE(100)\n",
        "params = vae.parameters()\n",
        "optimizer = Adam(params, lr=3e-4)\n",
        "print(vae)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VAE(\n",
            "  (encode): Encoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): Linear(in_features=784, out_features=300, bias=True)\n",
            "      (1): ELU(alpha=1.0)\n",
            "      (2): Linear(in_features=300, out_features=300, bias=True)\n",
            "      (3): ELU(alpha=1.0)\n",
            "      (4): Linear(in_features=300, out_features=200, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (decode): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): Linear(in_features=100, out_features=300, bias=True)\n",
            "      (1): ELU(alpha=1.0)\n",
            "      (2): Linear(in_features=300, out_features=300, bias=True)\n",
            "      (3): ELU(alpha=1.0)\n",
            "      (4): Linear(in_features=300, out_features=784, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqw9SI7aTdtG",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWtQakAOEhxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(20):\n",
        "    # train\n",
        "    for x in train:\n",
        "        optimizer.zero_grad()\n",
        "        z_mean, z_logvar, x_mean = vae(x)\n",
        "        loss = vae.loss(x, z_mean, z_logvar, x_mean)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # evaluate ELBO on the valid dataset\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.\n",
        "        total_count = 0\n",
        "        for x in valid:\n",
        "            total_loss += vae.loss(x, *vae(x)) * x.size(0)\n",
        "            total_count += x.size(0)\n",
        "        print('-elbo: ', (total_loss / total_count).item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXp5vuhDTg1J",
        "colab_type": "text"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYfmW5TAElEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(vae, 'model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iz6QX_KTizK",
        "colab_type": "text"
      },
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqcb8BrmEnMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = torch.load('model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTpoVRncTmSR",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the $\\log p_\\theta(x)$ of the model on test by using importance sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc2q6dxgEsIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_loss = 0.\n",
        "total_count = 0\n",
        "with torch.no_grad():\n",
        "    #x = next(iter(test))\n",
        "    for x in test:\n",
        "        # init\n",
        "        K = 200\n",
        "        M = x.size(0)\n",
        "\n",
        "        # Sample from the posterior\n",
        "        z_mean, z_logvar = vae.encode(x)\n",
        "        eps = torch.randn(z_mean.size(0), K, z_mean.size(1))\n",
        "        z_samples = z_mean[:, None, :] + torch.exp(z_logvar / 2.)[:, None, :] * eps # Broadcast the noise over the mean and variance\n",
        "\n",
        "        # Decode samples\n",
        "        z_samples_flat = z_samples.view(-1, z_samples.size(-1)) # Flatten out the z samples\n",
        "        x_mean_flat = vae.decode(z_samples_flat) # Push it through\n",
        "\n",
        "        # Reshape images and posterior to evaluate probabilities\n",
        "        x_flat = x[:, None].repeat(1, K, 1, 1, 1).reshape(M*K, -1)\n",
        "        z_mean_flat = z_mean[:, None, :].expand_as(z_samples).reshape(M*K, -1)\n",
        "        z_logvar_flat =  z_logvar[:, None, :].expand_as(z_samples).reshape(M*K, -1)\n",
        "        ZEROS = torch.zeros(z_mean_flat.size())\n",
        "\n",
        "        # Calculate all the probabilities!\n",
        "        log_p_x_z = log_likelihood_bernoulli(torch.sigmoid(x_mean_flat), x_flat).view(M, K)\n",
        "        log_q_z_x = log_likelihood_normal(z_mean_flat, z_logvar_flat, z_samples_flat).view(M, K)\n",
        "        log_p_z = log_likelihood_normal(ZEROS, ZEROS, z_samples_flat).view(M, K)\n",
        "\n",
        "        # Recombine them.\n",
        "        w = log_p_x_z + log_p_z - log_q_z_x\n",
        "        log_p = log_mean_exp(w)\n",
        "\n",
        "        # Accumulate\n",
        "        total_loss += log_p.sum()\n",
        "        total_count += M\n",
        "      \n",
        "print('log p(x):', (total_loss / total_count).item())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
